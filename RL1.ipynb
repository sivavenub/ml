{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = 'CartPole-v0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\win10\\Anaconda3\\envs\\ml\\lib\\site-packages\\gym\\envs\\registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(ENV_NAME)\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "nb_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                80        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 34        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 114\n",
      "Trainable params: 114\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 50000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\win10\\Anaconda3\\envs\\ml\\lib\\site-packages\\rl\\memory.py:39: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   140/50000: episode: 1, duration: 2.740s, episode steps: 140, steps per second: 51, episode reward: 140.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.246 [-1.449, 0.473], loss: 0.081287, mean_absolute_error: 11.107729, mean_q: 22.341499\n",
      "   317/50000: episode: 2, duration: 2.953s, episode steps: 177, steps per second: 60, episode reward: 177.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.199 [-1.628, 0.464], loss: 0.071130, mean_absolute_error: 10.638617, mean_q: 21.417234\n",
      "   448/50000: episode: 3, duration: 2.177s, episode steps: 131, steps per second: 60, episode reward: 131.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.248 [-1.470, 0.463], loss: 0.079393, mean_absolute_error: 10.911695, mean_q: 21.932398\n",
      "   573/50000: episode: 4, duration: 2.082s, episode steps: 125, steps per second: 60, episode reward: 125.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.222 [-1.292, 0.466], loss: 0.089158, mean_absolute_error: 11.162396, mean_q: 22.418560\n",
      "   694/50000: episode: 5, duration: 2.015s, episode steps: 121, steps per second: 60, episode reward: 121.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.233 [-1.593, 0.414], loss: 0.141366, mean_absolute_error: 11.488278, mean_q: 23.043270\n",
      "   766/50000: episode: 6, duration: 1.202s, episode steps: 72, steps per second: 60, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.143 [-0.282, 0.888], loss: 0.189908, mean_absolute_error: 11.856600, mean_q: 23.722481\n",
      "   918/50000: episode: 7, duration: 2.532s, episode steps: 152, steps per second: 60, episode reward: 152.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.172 [-1.479, 0.515], loss: 0.600896, mean_absolute_error: 12.445401, mean_q: 24.863384\n",
      "  1055/50000: episode: 8, duration: 2.279s, episode steps: 137, steps per second: 60, episode reward: 137.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.179 [-1.298, 0.411], loss: 0.473173, mean_absolute_error: 12.802857, mean_q: 25.577961\n",
      "  1157/50000: episode: 9, duration: 1.699s, episode steps: 102, steps per second: 60, episode reward: 102.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.145 [-0.358, 1.121], loss: 0.708622, mean_absolute_error: 13.194289, mean_q: 26.299643\n",
      "  1267/50000: episode: 10, duration: 1.830s, episode steps: 110, steps per second: 60, episode reward: 110.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.213 [-1.299, 0.355], loss: 1.242954, mean_absolute_error: 13.769111, mean_q: 27.434820\n",
      "  1372/50000: episode: 11, duration: 1.744s, episode steps: 105, steps per second: 60, episode reward: 105.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.135 [-0.314, 0.939], loss: 1.139054, mean_absolute_error: 13.953382, mean_q: 27.750011\n",
      "  1504/50000: episode: 12, duration: 2.205s, episode steps: 132, steps per second: 60, episode reward: 132.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.205 [-1.329, 0.596], loss: 1.453325, mean_absolute_error: 14.403326, mean_q: 28.660305\n",
      "  1620/50000: episode: 13, duration: 1.933s, episode steps: 116, steps per second: 60, episode reward: 116.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.218 [-1.274, 0.465], loss: 0.792738, mean_absolute_error: 14.666880, mean_q: 29.224895\n",
      "  1765/50000: episode: 14, duration: 2.413s, episode steps: 145, steps per second: 60, episode reward: 145.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.184 [-1.254, 0.607], loss: 0.871587, mean_absolute_error: 14.907765, mean_q: 29.731472\n",
      "  1965/50000: episode: 15, duration: 3.333s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.058 [-0.674, 0.711], loss: 1.124060, mean_absolute_error: 15.225685, mean_q: 30.340479\n",
      "  2040/50000: episode: 16, duration: 1.246s, episode steps: 75, steps per second: 60, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.233 [-0.403, 1.135], loss: 1.202701, mean_absolute_error: 15.845454, mean_q: 31.536860\n",
      "  2216/50000: episode: 17, duration: 2.927s, episode steps: 176, steps per second: 60, episode reward: 176.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.195 [-1.669, 0.616], loss: 1.565439, mean_absolute_error: 16.191198, mean_q: 32.241154\n",
      "  2356/50000: episode: 18, duration: 2.341s, episode steps: 140, steps per second: 60, episode reward: 140.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.255 [-1.835, 0.595], loss: 1.720240, mean_absolute_error: 16.278814, mean_q: 32.393208\n",
      "  2554/50000: episode: 19, duration: 3.299s, episode steps: 198, steps per second: 60, episode reward: 198.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.197 [-1.785, 0.418], loss: 1.051196, mean_absolute_error: 16.649101, mean_q: 33.236679\n",
      "  2732/50000: episode: 20, duration: 2.964s, episode steps: 178, steps per second: 60, episode reward: 178.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.247 [-1.872, 0.376], loss: 1.936122, mean_absolute_error: 16.880569, mean_q: 33.597378\n",
      "  2917/50000: episode: 21, duration: 3.084s, episode steps: 185, steps per second: 60, episode reward: 185.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.239 [-2.040, 0.667], loss: 1.029639, mean_absolute_error: 17.138630, mean_q: 34.213947\n",
      "  3082/50000: episode: 22, duration: 2.746s, episode steps: 165, steps per second: 60, episode reward: 165.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.527 [0.000, 1.000], mean observation: 0.179 [-0.374, 1.595], loss: 0.608964, mean_absolute_error: 17.422449, mean_q: 34.823959\n",
      "  3271/50000: episode: 23, duration: 3.147s, episode steps: 189, steps per second: 60, episode reward: 189.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.226 [-1.969, 0.824], loss: 1.489241, mean_absolute_error: 17.969034, mean_q: 35.834492\n",
      "  3401/50000: episode: 24, duration: 2.165s, episode steps: 130, steps per second: 60, episode reward: 130.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.206 [-0.417, 1.466], loss: 1.104530, mean_absolute_error: 18.153498, mean_q: 36.227150\n",
      "  3574/50000: episode: 25, duration: 2.885s, episode steps: 173, steps per second: 60, episode reward: 173.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.262 [-1.966, 0.463], loss: 1.631345, mean_absolute_error: 18.398640, mean_q: 36.676460\n",
      "  3690/50000: episode: 26, duration: 1.931s, episode steps: 116, steps per second: 60, episode reward: 116.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.534 [0.000, 1.000], mean observation: 0.211 [-0.521, 1.456], loss: 1.439096, mean_absolute_error: 18.576128, mean_q: 37.051384\n",
      "  3848/50000: episode: 27, duration: 2.629s, episode steps: 158, steps per second: 60, episode reward: 158.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.282 [-2.032, 0.654], loss: 0.757929, mean_absolute_error: 18.910107, mean_q: 37.768581\n",
      "  4013/50000: episode: 28, duration: 2.754s, episode steps: 165, steps per second: 60, episode reward: 165.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.286 [-2.046, 0.512], loss: 1.643747, mean_absolute_error: 19.143259, mean_q: 38.164909\n",
      "  4159/50000: episode: 29, duration: 2.433s, episode steps: 146, steps per second: 60, episode reward: 146.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.314 [-1.804, 0.491], loss: 1.220074, mean_absolute_error: 19.166655, mean_q: 38.232761\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4314/50000: episode: 30, duration: 2.582s, episode steps: 155, steps per second: 60, episode reward: 155.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.312 [-1.992, 0.582], loss: 1.013319, mean_absolute_error: 19.370586, mean_q: 38.675415\n",
      "  4454/50000: episode: 31, duration: 2.334s, episode steps: 140, steps per second: 60, episode reward: 140.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.150 [-0.531, 1.414], loss: 2.442515, mean_absolute_error: 19.563305, mean_q: 38.929066\n",
      "  4629/50000: episode: 32, duration: 2.909s, episode steps: 175, steps per second: 60, episode reward: 175.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.305 [-2.016, 0.460], loss: 1.547242, mean_absolute_error: 19.825495, mean_q: 39.507526\n",
      "  4732/50000: episode: 33, duration: 1.719s, episode steps: 103, steps per second: 60, episode reward: 103.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.534 [0.000, 1.000], mean observation: 0.193 [-0.339, 1.313], loss: 1.397227, mean_absolute_error: 19.900623, mean_q: 39.712566\n",
      "  4913/50000: episode: 34, duration: 3.012s, episode steps: 181, steps per second: 60, episode reward: 181.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.106 [-0.674, 1.248], loss: 1.573991, mean_absolute_error: 20.238714, mean_q: 40.318661\n",
      "  5062/50000: episode: 35, duration: 2.486s, episode steps: 149, steps per second: 60, episode reward: 149.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.340 [-1.949, 0.439], loss: 1.189663, mean_absolute_error: 20.517656, mean_q: 40.946209\n",
      "  5157/50000: episode: 36, duration: 1.579s, episode steps: 95, steps per second: 60, episode reward: 95.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.171 [-0.356, 0.912], loss: 1.890212, mean_absolute_error: 20.654406, mean_q: 41.175667\n",
      "  5315/50000: episode: 37, duration: 2.633s, episode steps: 158, steps per second: 60, episode reward: 158.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.317 [-1.830, 0.571], loss: 0.938591, mean_absolute_error: 20.871349, mean_q: 41.666370\n",
      "  5500/50000: episode: 38, duration: 3.081s, episode steps: 185, steps per second: 60, episode reward: 185.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.363 [-2.419, 0.568], loss: 1.409237, mean_absolute_error: 21.178602, mean_q: 42.247906\n",
      "  5670/50000: episode: 39, duration: 2.831s, episode steps: 170, steps per second: 60, episode reward: 170.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.284 [-1.717, 0.585], loss: 1.212038, mean_absolute_error: 21.310823, mean_q: 42.501808\n",
      "  5837/50000: episode: 40, duration: 2.786s, episode steps: 167, steps per second: 60, episode reward: 167.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.372 [-2.242, 0.558], loss: 1.192043, mean_absolute_error: 21.431110, mean_q: 42.742210\n",
      "  6016/50000: episode: 41, duration: 2.983s, episode steps: 179, steps per second: 60, episode reward: 179.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.362 [-2.341, 0.733], loss: 1.172116, mean_absolute_error: 21.684591, mean_q: 43.258297\n",
      "  6126/50000: episode: 42, duration: 1.832s, episode steps: 110, steps per second: 60, episode reward: 110.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.527 [0.000, 1.000], mean observation: 0.180 [-0.546, 1.131], loss: 1.133056, mean_absolute_error: 21.782375, mean_q: 43.367573\n",
      "  6301/50000: episode: 43, duration: 2.916s, episode steps: 175, steps per second: 60, episode reward: 175.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.374 [-2.408, 0.740], loss: 1.354629, mean_absolute_error: 21.810532, mean_q: 43.426464\n",
      "  6501/50000: episode: 44, duration: 3.330s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.146 [-1.150, 0.603], loss: 1.012495, mean_absolute_error: 22.078472, mean_q: 43.975662\n",
      "  6661/50000: episode: 45, duration: 2.667s, episode steps: 160, steps per second: 60, episode reward: 160.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.306 [-1.816, 1.273], loss: 1.082961, mean_absolute_error: 22.355442, mean_q: 44.478680\n",
      "  6861/50000: episode: 46, duration: 3.330s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.319 [-2.338, 0.528], loss: 0.956115, mean_absolute_error: 22.616749, mean_q: 45.041122\n",
      "  6995/50000: episode: 47, duration: 2.231s, episode steps: 134, steps per second: 60, episode reward: 134.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.176 [-0.396, 1.117], loss: 0.550681, mean_absolute_error: 22.531178, mean_q: 44.908169\n",
      "  7195/50000: episode: 48, duration: 3.330s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.262 [-1.910, 0.571], loss: 1.326921, mean_absolute_error: 22.881123, mean_q: 45.550037\n",
      "  7395/50000: episode: 49, duration: 3.335s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.284 [-2.030, 0.569], loss: 0.877717, mean_absolute_error: 23.160891, mean_q: 46.189873\n",
      "  7595/50000: episode: 50, duration: 3.328s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.327 [-2.251, 0.613], loss: 1.016080, mean_absolute_error: 23.391157, mean_q: 46.641869\n",
      "  7795/50000: episode: 51, duration: 3.334s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.262 [-1.788, 1.174], loss: 0.885376, mean_absolute_error: 23.707644, mean_q: 47.290871\n",
      "  7995/50000: episode: 52, duration: 3.333s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.354 [-2.341, 0.587], loss: 0.973367, mean_absolute_error: 23.859337, mean_q: 47.599335\n",
      "  8195/50000: episode: 53, duration: 3.346s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.258 [-1.762, 0.633], loss: 0.740105, mean_absolute_error: 24.000280, mean_q: 47.925636\n",
      "  8395/50000: episode: 54, duration: 3.332s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.302 [-2.005, 0.845], loss: 0.923418, mean_absolute_error: 24.303495, mean_q: 48.512104\n",
      "  8595/50000: episode: 55, duration: 3.331s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.190 [-1.289, 0.521], loss: 0.786527, mean_absolute_error: 24.596413, mean_q: 49.083702\n",
      "  8795/50000: episode: 56, duration: 3.331s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.106 [-0.690, 0.592], loss: 1.263728, mean_absolute_error: 24.865278, mean_q: 49.623123\n",
      "  8995/50000: episode: 57, duration: 3.334s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.350 [-2.359, 1.219], loss: 1.433185, mean_absolute_error: 25.118755, mean_q: 50.039326\n",
      "  9195/50000: episode: 58, duration: 3.332s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.188 [-1.116, 0.667], loss: 0.754399, mean_absolute_error: 25.293100, mean_q: 50.488651\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  9395/50000: episode: 59, duration: 3.332s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.251 [-1.532, 0.857], loss: 1.774535, mean_absolute_error: 25.724499, mean_q: 51.321415\n",
      "  9595/50000: episode: 60, duration: 3.329s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.187 [-1.122, 0.902], loss: 1.233495, mean_absolute_error: 25.831139, mean_q: 51.569195\n",
      "  9795/50000: episode: 61, duration: 3.330s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.104 [-0.735, 0.551], loss: 0.880468, mean_absolute_error: 26.018377, mean_q: 51.972919\n",
      "  9995/50000: episode: 62, duration: 3.335s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.205 [-1.178, 0.636], loss: 0.759376, mean_absolute_error: 26.366924, mean_q: 52.683151\n",
      " 10195/50000: episode: 63, duration: 3.347s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.530 [0.000, 1.000], mean observation: 0.276 [-0.898, 2.453], loss: 1.049416, mean_absolute_error: 26.665033, mean_q: 53.270920\n",
      " 10395/50000: episode: 64, duration: 3.331s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.133 [-0.956, 0.929], loss: 1.952413, mean_absolute_error: 26.953598, mean_q: 53.794121\n",
      " 10583/50000: episode: 65, duration: 3.135s, episode steps: 188, steps per second: 60, episode reward: 188.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.197 [-2.442, 2.451], loss: 1.332493, mean_absolute_error: 27.300323, mean_q: 54.488781\n",
      " 10652/50000: episode: 66, duration: 1.142s, episode steps: 69, steps per second: 60, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: -0.103 [-2.398, 1.230], loss: 1.211195, mean_absolute_error: 27.504822, mean_q: 54.926155\n",
      " 10817/50000: episode: 67, duration: 2.749s, episode steps: 165, steps per second: 60, episode reward: 165.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: -0.153 [-2.830, 1.437], loss: 1.751067, mean_absolute_error: 27.733250, mean_q: 55.292843\n",
      " 10859/50000: episode: 68, duration: 0.698s, episode steps: 42, steps per second: 60, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.086 [-1.713, 0.824], loss: 1.873357, mean_absolute_error: 27.854954, mean_q: 55.527718\n",
      " 10899/50000: episode: 69, duration: 0.666s, episode steps: 40, steps per second: 60, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.625 [0.000, 1.000], mean observation: -0.111 [-3.418, 1.961], loss: 0.931046, mean_absolute_error: 27.745981, mean_q: 55.341797\n",
      " 11099/50000: episode: 70, duration: 3.331s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.193 [-1.058, 0.638], loss: 1.867868, mean_absolute_error: 27.901165, mean_q: 55.640820\n",
      " 11299/50000: episode: 71, duration: 3.332s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.093 [-0.977, 0.888], loss: 1.458943, mean_absolute_error: 28.272589, mean_q: 56.546925\n",
      " 11499/50000: episode: 72, duration: 3.333s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.069 [-0.628, 0.669], loss: 2.345978, mean_absolute_error: 28.870308, mean_q: 57.692352\n",
      " 11699/50000: episode: 73, duration: 3.333s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.070 [-0.595, 1.113], loss: 2.352091, mean_absolute_error: 29.198706, mean_q: 58.444767\n",
      " 11883/50000: episode: 74, duration: 3.063s, episode steps: 184, steps per second: 60, episode reward: 184.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.527 [0.000, 1.000], mean observation: 0.318 [-1.316, 2.584], loss: 3.782952, mean_absolute_error: 29.590124, mean_q: 59.235619\n",
      " 12013/50000: episode: 75, duration: 2.165s, episode steps: 130, steps per second: 60, episode reward: 130.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.424 [-0.884, 2.419], loss: 3.048980, mean_absolute_error: 29.920132, mean_q: 60.017841\n",
      " 12134/50000: episode: 76, duration: 2.013s, episode steps: 121, steps per second: 60, episode reward: 121.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.449 [-0.750, 2.435], loss: 3.110624, mean_absolute_error: 30.205614, mean_q: 60.747543\n",
      " 12255/50000: episode: 77, duration: 2.017s, episode steps: 121, steps per second: 60, episode reward: 121.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.451 [-1.500, 2.778], loss: 2.620496, mean_absolute_error: 30.433167, mean_q: 61.263165\n",
      " 12377/50000: episode: 78, duration: 2.030s, episode steps: 122, steps per second: 60, episode reward: 122.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.541 [0.000, 1.000], mean observation: 0.455 [-0.718, 2.408], loss: 1.445323, mean_absolute_error: 30.713703, mean_q: 61.835789\n",
      " 12519/50000: episode: 79, duration: 2.367s, episode steps: 142, steps per second: 60, episode reward: 142.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.410 [-1.053, 2.429], loss: 2.802047, mean_absolute_error: 31.131948, mean_q: 62.598511\n",
      " 12652/50000: episode: 80, duration: 2.216s, episode steps: 133, steps per second: 60, episode reward: 133.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.534 [0.000, 1.000], mean observation: 0.444 [-0.925, 2.423], loss: 3.884885, mean_absolute_error: 31.309204, mean_q: 62.745403\n",
      " 12820/50000: episode: 81, duration: 2.797s, episode steps: 168, steps per second: 60, episode reward: 168.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.530 [0.000, 1.000], mean observation: 0.358 [-0.733, 2.415], loss: 3.669413, mean_absolute_error: 31.683237, mean_q: 63.447823\n",
      " 12985/50000: episode: 82, duration: 2.749s, episode steps: 165, steps per second: 60, episode reward: 165.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.527 [0.000, 1.000], mean observation: 0.368 [-0.967, 2.401], loss: 3.878316, mean_absolute_error: 31.715151, mean_q: 63.515530\n",
      " 13127/50000: episode: 83, duration: 2.364s, episode steps: 142, steps per second: 60, episode reward: 142.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.423 [-0.542, 2.404], loss: 4.574665, mean_absolute_error: 32.135181, mean_q: 64.355530\n",
      " 13312/50000: episode: 84, duration: 3.083s, episode steps: 185, steps per second: 60, episode reward: 185.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.328 [-0.715, 2.425], loss: 2.554293, mean_absolute_error: 32.447525, mean_q: 65.190659\n",
      " 13462/50000: episode: 85, duration: 2.514s, episode steps: 150, steps per second: 60, episode reward: 150.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.434 [-0.601, 2.406], loss: 5.065543, mean_absolute_error: 32.836678, mean_q: 65.845016\n",
      " 13595/50000: episode: 86, duration: 2.215s, episode steps: 133, steps per second: 60, episode reward: 133.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.442 [-0.693, 2.415], loss: 4.718870, mean_absolute_error: 33.149891, mean_q: 66.465721\n",
      " 13759/50000: episode: 87, duration: 2.731s, episode steps: 164, steps per second: 60, episode reward: 164.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.399 [-0.533, 2.405], loss: 4.179265, mean_absolute_error: 33.403179, mean_q: 66.997406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 13926/50000: episode: 88, duration: 2.782s, episode steps: 167, steps per second: 60, episode reward: 167.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.384 [-0.538, 2.417], loss: 3.932365, mean_absolute_error: 33.444031, mean_q: 67.134026\n",
      " 14107/50000: episode: 89, duration: 3.014s, episode steps: 181, steps per second: 60, episode reward: 181.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.380 [-0.689, 2.408], loss: 5.925404, mean_absolute_error: 33.994556, mean_q: 68.116722\n",
      " 14307/50000: episode: 90, duration: 3.331s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.292 [-0.588, 1.969], loss: 5.582488, mean_absolute_error: 34.387798, mean_q: 69.153435\n",
      " 14507/50000: episode: 91, duration: 3.335s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.193 [-0.788, 1.344], loss: 7.026785, mean_absolute_error: 35.286087, mean_q: 70.794006\n",
      " 14707/50000: episode: 92, duration: 3.328s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.346 [-0.611, 2.335], loss: 7.258113, mean_absolute_error: 35.618549, mean_q: 71.476181\n",
      " 14907/50000: episode: 93, duration: 3.332s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.176 [-0.544, 1.143], loss: 7.765119, mean_absolute_error: 36.102474, mean_q: 72.395424\n",
      " 15107/50000: episode: 94, duration: 3.332s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.203 [-0.529, 1.326], loss: 6.006744, mean_absolute_error: 36.649227, mean_q: 73.366821\n",
      " 15307/50000: episode: 95, duration: 3.330s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.071 [-0.853, 0.782], loss: 5.997555, mean_absolute_error: 36.720146, mean_q: 73.670319\n",
      " 15507/50000: episode: 96, duration: 3.332s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.142 [-0.600, 1.081], loss: 8.702001, mean_absolute_error: 37.579586, mean_q: 75.311920\n",
      " 15707/50000: episode: 97, duration: 3.331s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.068 [-0.524, 0.559], loss: 9.176849, mean_absolute_error: 37.709785, mean_q: 75.493034\n",
      " 15907/50000: episode: 98, duration: 3.332s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.079 [-0.544, 0.565], loss: 8.661037, mean_absolute_error: 37.871170, mean_q: 75.832405\n",
      " 16107/50000: episode: 99, duration: 3.331s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.155 [-0.519, 0.881], loss: 8.459685, mean_absolute_error: 38.634472, mean_q: 77.387535\n",
      " 16307/50000: episode: 100, duration: 3.333s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.098 [-0.616, 0.484], loss: 8.115752, mean_absolute_error: 38.857925, mean_q: 77.831497\n",
      " 16507/50000: episode: 101, duration: 3.330s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.016 [-0.524, 0.479], loss: 13.329399, mean_absolute_error: 39.232616, mean_q: 78.455292\n",
      " 16707/50000: episode: 102, duration: 3.332s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.131 [-0.783, 0.752], loss: 10.578285, mean_absolute_error: 39.382442, mean_q: 78.793793\n",
      " 16907/50000: episode: 103, duration: 3.332s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.014 [-0.605, 0.565], loss: 8.168543, mean_absolute_error: 39.806873, mean_q: 79.816750\n",
      " 17107/50000: episode: 104, duration: 3.331s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.052 [-0.774, 0.614], loss: 10.415844, mean_absolute_error: 40.126293, mean_q: 80.391800\n",
      " 17307/50000: episode: 105, duration: 3.330s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.140 [-0.811, 0.689], loss: 12.156816, mean_absolute_error: 40.769176, mean_q: 81.673576\n",
      " 17507/50000: episode: 106, duration: 3.333s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.124 [-0.623, 0.626], loss: 13.400186, mean_absolute_error: 40.960430, mean_q: 81.977638\n",
      " 17707/50000: episode: 107, duration: 3.332s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.137 [-0.813, 0.559], loss: 10.505637, mean_absolute_error: 41.361893, mean_q: 82.920448\n",
      " 17907/50000: episode: 108, duration: 3.329s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.160 [-0.896, 0.617], loss: 11.330754, mean_absolute_error: 41.664570, mean_q: 83.480446\n",
      " 18107/50000: episode: 109, duration: 3.333s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.149 [-0.852, 0.620], loss: 11.000710, mean_absolute_error: 42.002254, mean_q: 84.170563\n",
      " 18307/50000: episode: 110, duration: 3.328s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.142 [-0.898, 0.597], loss: 15.500564, mean_absolute_error: 42.147984, mean_q: 84.357620\n",
      " 18507/50000: episode: 111, duration: 3.334s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.149 [-0.767, 0.785], loss: 13.160640, mean_absolute_error: 42.640743, mean_q: 85.295380\n",
      " 18707/50000: episode: 112, duration: 3.347s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.159 [-0.958, 0.768], loss: 16.361162, mean_absolute_error: 42.784412, mean_q: 85.474335\n",
      " 18907/50000: episode: 113, duration: 3.332s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.205 [-0.955, 0.437], loss: 13.488623, mean_absolute_error: 42.749287, mean_q: 85.611961\n",
      " 19107/50000: episode: 114, duration: 3.329s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.189 [-1.087, 0.796], loss: 15.782336, mean_absolute_error: 43.398556, mean_q: 86.875603\n",
      " 19307/50000: episode: 115, duration: 3.333s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.198 [-0.993, 0.623], loss: 12.865038, mean_absolute_error: 43.663517, mean_q: 87.512306\n",
      " 19507/50000: episode: 116, duration: 3.331s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.170 [-0.907, 0.697], loss: 15.932251, mean_absolute_error: 43.924294, mean_q: 87.958046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 19707/50000: episode: 117, duration: 3.332s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.172 [-1.265, 0.897], loss: 15.698481, mean_absolute_error: 43.958954, mean_q: 88.005203\n",
      " 19907/50000: episode: 118, duration: 3.331s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.169 [-0.811, 0.624], loss: 20.508041, mean_absolute_error: 44.271420, mean_q: 88.446991\n",
      " 20107/50000: episode: 119, duration: 3.329s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.158 [-0.906, 0.617], loss: 19.659630, mean_absolute_error: 44.378292, mean_q: 88.742302\n",
      " 20307/50000: episode: 120, duration: 3.332s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.184 [-0.927, 0.524], loss: 20.969280, mean_absolute_error: 44.492996, mean_q: 88.884567\n",
      " 20507/50000: episode: 121, duration: 3.334s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.192 [-0.975, 0.660], loss: 12.729235, mean_absolute_error: 44.706837, mean_q: 89.557693\n",
      " 20707/50000: episode: 122, duration: 3.332s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.221 [-1.080, 0.714], loss: 16.435198, mean_absolute_error: 44.746178, mean_q: 89.554138\n",
      " 20907/50000: episode: 123, duration: 3.330s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.223 [-1.126, 0.876], loss: 19.053261, mean_absolute_error: 45.183304, mean_q: 90.401413\n",
      " 21107/50000: episode: 124, duration: 3.332s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.231 [-1.104, 0.738], loss: 10.516328, mean_absolute_error: 45.362030, mean_q: 90.927528\n",
      " 21307/50000: episode: 125, duration: 3.330s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.198 [-0.994, 0.720], loss: 15.989279, mean_absolute_error: 45.478291, mean_q: 90.980400\n",
      " 21507/50000: episode: 126, duration: 3.333s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.219 [-1.114, 0.642], loss: 19.068556, mean_absolute_error: 45.938370, mean_q: 91.850517\n",
      " 21707/50000: episode: 127, duration: 3.332s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.199 [-0.971, 0.624], loss: 20.122011, mean_absolute_error: 46.132736, mean_q: 92.180351\n",
      " 21907/50000: episode: 128, duration: 3.330s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.212 [-1.119, 0.821], loss: 19.460018, mean_absolute_error: 46.350574, mean_q: 92.682152\n",
      " 22107/50000: episode: 129, duration: 3.331s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.207 [-1.090, 0.865], loss: 26.419222, mean_absolute_error: 46.198849, mean_q: 92.168808\n",
      " 22307/50000: episode: 130, duration: 3.332s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.215 [-1.116, 0.691], loss: 15.172552, mean_absolute_error: 46.323891, mean_q: 92.700546\n",
      " 22507/50000: episode: 131, duration: 3.333s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.207 [-1.042, 0.562], loss: 20.507143, mean_absolute_error: 46.510574, mean_q: 92.973785\n",
      " 22707/50000: episode: 132, duration: 3.328s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.221 [-1.126, 0.722], loss: 21.743593, mean_absolute_error: 46.647324, mean_q: 93.181000\n",
      " 22907/50000: episode: 133, duration: 3.349s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.230 [-1.114, 0.626], loss: 21.148916, mean_absolute_error: 46.673885, mean_q: 93.239082\n",
      " 23107/50000: episode: 134, duration: 3.334s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.224 [-1.327, 0.863], loss: 15.569209, mean_absolute_error: 46.920048, mean_q: 93.894928\n",
      " 23307/50000: episode: 135, duration: 3.335s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.205 [-1.038, 0.856], loss: 17.423523, mean_absolute_error: 47.186447, mean_q: 94.370705\n",
      " 23507/50000: episode: 136, duration: 3.333s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.241 [-1.262, 0.822], loss: 20.968847, mean_absolute_error: 47.360607, mean_q: 94.602600\n",
      " 23707/50000: episode: 137, duration: 3.324s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.220 [-1.093, 0.614], loss: 26.556328, mean_absolute_error: 47.419647, mean_q: 94.559464\n",
      " 23907/50000: episode: 138, duration: 3.337s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.261 [-1.505, 1.033], loss: 21.962889, mean_absolute_error: 47.399036, mean_q: 94.650009\n",
      " 24107/50000: episode: 139, duration: 3.330s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.242 [-1.302, 0.861], loss: 23.044849, mean_absolute_error: 47.639496, mean_q: 95.122269\n",
      " 24307/50000: episode: 140, duration: 3.332s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.249 [-1.171, 0.768], loss: 12.744633, mean_absolute_error: 47.466827, mean_q: 95.023682\n",
      " 24507/50000: episode: 141, duration: 3.334s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.221 [-1.103, 0.624], loss: 17.788853, mean_absolute_error: 47.859913, mean_q: 95.623734\n",
      " 24707/50000: episode: 142, duration: 3.331s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.215 [-1.140, 0.657], loss: 19.240433, mean_absolute_error: 47.869545, mean_q: 95.581306\n",
      " 24907/50000: episode: 143, duration: 3.337s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.234 [-1.114, 0.575], loss: 18.524986, mean_absolute_error: 48.235600, mean_q: 96.395706\n",
      " 25107/50000: episode: 144, duration: 3.325s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.235 [-1.153, 0.636], loss: 16.816824, mean_absolute_error: 48.192120, mean_q: 96.404358\n",
      " 25307/50000: episode: 145, duration: 3.326s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.216 [-1.031, 0.596], loss: 15.138928, mean_absolute_error: 48.400902, mean_q: 96.820160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 25507/50000: episode: 146, duration: 3.342s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.241 [-1.191, 0.645], loss: 23.817211, mean_absolute_error: 48.456268, mean_q: 96.830643\n",
      " 25707/50000: episode: 147, duration: 3.333s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.250 [-1.478, 0.973], loss: 14.747516, mean_absolute_error: 48.517372, mean_q: 97.230881\n",
      " 25907/50000: episode: 148, duration: 3.331s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.249 [-1.311, 0.812], loss: 18.679228, mean_absolute_error: 48.658459, mean_q: 97.427666\n",
      " 26107/50000: episode: 149, duration: 3.326s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.258 [-1.276, 0.683], loss: 14.110329, mean_absolute_error: 48.732704, mean_q: 97.645233\n",
      " 26307/50000: episode: 150, duration: 3.337s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.281 [-1.455, 0.878], loss: 18.114788, mean_absolute_error: 49.186329, mean_q: 98.614037\n",
      " 26507/50000: episode: 151, duration: 3.333s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.313 [-1.670, 1.183], loss: 20.390688, mean_absolute_error: 49.627872, mean_q: 99.456169\n",
      " 26707/50000: episode: 152, duration: 3.326s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.327 [-1.697, 1.029], loss: 24.027094, mean_absolute_error: 49.987991, mean_q: 100.113792\n",
      " 26723/50000: episode: 153, duration: 0.261s, episode steps: 16, steps per second: 61, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.101 [-1.196, 0.754], loss: 9.987478, mean_absolute_error: 50.026039, mean_q: 100.453255\n",
      " 26923/50000: episode: 154, duration: 3.341s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.352 [-1.620, 0.789], loss: 19.737581, mean_absolute_error: 50.231796, mean_q: 100.745911\n",
      " 27123/50000: episode: 155, duration: 3.332s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.337 [-1.660, 0.714], loss: 19.082207, mean_absolute_error: 50.686180, mean_q: 101.635460\n",
      " 27146/50000: episode: 156, duration: 0.371s, episode steps: 23, steps per second: 62, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.105 [-0.991, 0.630], loss: 34.679661, mean_absolute_error: 50.583263, mean_q: 101.076736\n",
      " 27162/50000: episode: 157, duration: 0.268s, episode steps: 16, steps per second: 60, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.074 [-1.501, 0.995], loss: 5.266338, mean_absolute_error: 51.026913, mean_q: 102.719872\n",
      " 27177/50000: episode: 158, duration: 0.247s, episode steps: 15, steps per second: 61, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.088 [-1.199, 0.829], loss: 24.159502, mean_absolute_error: 50.961136, mean_q: 102.171593\n",
      " 27377/50000: episode: 159, duration: 3.337s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.343 [-1.659, 0.865], loss: 21.473661, mean_absolute_error: 50.976906, mean_q: 102.195198\n",
      " 27389/50000: episode: 160, duration: 0.202s, episode steps: 12, steps per second: 59, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.125 [-1.585, 0.942], loss: 50.506748, mean_absolute_error: 52.110691, mean_q: 103.829620\n",
      " 27409/50000: episode: 161, duration: 0.329s, episode steps: 20, steps per second: 61, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: -0.086 [-1.156, 0.770], loss: 21.772173, mean_absolute_error: 51.024300, mean_q: 102.118294\n",
      " 27609/50000: episode: 162, duration: 3.332s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.350 [-1.770, 0.957], loss: 26.679136, mean_absolute_error: 51.316540, mean_q: 102.724373\n",
      " 27623/50000: episode: 163, duration: 0.229s, episode steps: 14, steps per second: 61, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.082 [-1.509, 0.954], loss: 32.324238, mean_absolute_error: 50.941570, mean_q: 102.018250\n",
      " 27642/50000: episode: 164, duration: 0.313s, episode steps: 19, steps per second: 61, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.084 [-1.181, 0.826], loss: 7.438036, mean_absolute_error: 51.666737, mean_q: 103.896385\n",
      " 27842/50000: episode: 165, duration: 3.336s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.410 [-1.932, 0.874], loss: 19.548113, mean_absolute_error: 51.685116, mean_q: 103.776169\n",
      " 27864/50000: episode: 166, duration: 0.368s, episode steps: 22, steps per second: 60, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.122 [-0.974, 0.606], loss: 40.662899, mean_absolute_error: 51.955704, mean_q: 103.841530\n",
      " 28064/50000: episode: 167, duration: 3.337s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.371 [-1.808, 0.837], loss: 27.691273, mean_absolute_error: 51.932724, mean_q: 103.885002\n",
      " 28264/50000: episode: 168, duration: 3.320s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.368 [-1.917, 1.323], loss: 23.979111, mean_absolute_error: 52.076027, mean_q: 104.203362\n",
      " 28464/50000: episode: 169, duration: 3.342s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.358 [-1.765, 1.016], loss: 28.252060, mean_absolute_error: 52.325371, mean_q: 104.695099\n",
      " 28483/50000: episode: 170, duration: 0.306s, episode steps: 19, steps per second: 62, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: -0.114 [-1.059, 0.637], loss: 7.933214, mean_absolute_error: 51.832451, mean_q: 104.278137\n",
      " 28683/50000: episode: 171, duration: 3.334s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.359 [-1.737, 0.950], loss: 21.669588, mean_absolute_error: 52.585350, mean_q: 105.389542\n",
      " 28883/50000: episode: 172, duration: 3.337s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.426 [-2.115, 0.801], loss: 32.111916, mean_absolute_error: 52.672733, mean_q: 105.450134\n",
      " 29083/50000: episode: 173, duration: 3.331s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.341 [-1.703, 0.929], loss: 23.088856, mean_absolute_error: 52.858994, mean_q: 105.716736\n",
      " 29283/50000: episode: 174, duration: 3.331s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.315 [-1.620, 0.984], loss: 30.888836, mean_absolute_error: 52.819260, mean_q: 105.414398\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 29483/50000: episode: 175, duration: 3.321s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.366 [-1.797, 0.828], loss: 19.865673, mean_absolute_error: 52.835850, mean_q: 105.864929\n",
      " 29683/50000: episode: 176, duration: 3.333s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.345 [-1.749, 0.818], loss: 22.511065, mean_absolute_error: 53.000240, mean_q: 106.197304\n",
      " 29883/50000: episode: 177, duration: 3.337s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.348 [-1.847, 0.823], loss: 21.678032, mean_absolute_error: 53.163605, mean_q: 106.501053\n",
      " 30083/50000: episode: 178, duration: 3.331s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.371 [-1.859, 0.855], loss: 25.566046, mean_absolute_error: 53.160870, mean_q: 106.368378\n",
      " 30283/50000: episode: 179, duration: 3.335s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.380 [-2.005, 0.718], loss: 14.744970, mean_absolute_error: 53.400146, mean_q: 107.205193\n",
      " 30483/50000: episode: 180, duration: 3.320s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.391 [-2.105, 0.767], loss: 24.652349, mean_absolute_error: 53.438892, mean_q: 107.068069\n",
      " 30501/50000: episode: 181, duration: 0.305s, episode steps: 18, steps per second: 59, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.100 [-0.988, 0.598], loss: 1.495630, mean_absolute_error: 52.900936, mean_q: 106.646782\n",
      " 30701/50000: episode: 182, duration: 3.327s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.401 [-2.018, 0.771], loss: 22.970827, mean_absolute_error: 53.672218, mean_q: 107.654762\n",
      " 30901/50000: episode: 183, duration: 3.337s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.376 [-1.954, 0.864], loss: 26.962566, mean_absolute_error: 53.899994, mean_q: 107.930527\n",
      " 31101/50000: episode: 184, duration: 3.337s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.423 [-2.227, 0.659], loss: 32.157661, mean_absolute_error: 53.948097, mean_q: 108.070267\n",
      " 31301/50000: episode: 185, duration: 3.330s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.389 [-2.066, 1.006], loss: 23.247673, mean_absolute_error: 53.924347, mean_q: 108.083832\n",
      " 31501/50000: episode: 186, duration: 3.335s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.441 [-2.334, 0.884], loss: 28.985353, mean_absolute_error: 53.953369, mean_q: 108.015213\n",
      " 31701/50000: episode: 187, duration: 3.324s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.371 [-1.966, 0.738], loss: 20.982107, mean_absolute_error: 54.156952, mean_q: 108.535576\n",
      " 31901/50000: episode: 188, duration: 3.330s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.360 [-2.002, 0.862], loss: 32.184830, mean_absolute_error: 54.320045, mean_q: 108.686325\n",
      " 32101/50000: episode: 189, duration: 3.340s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.401 [-2.183, 0.787], loss: 15.265334, mean_absolute_error: 54.228798, mean_q: 108.882462\n",
      " 32301/50000: episode: 190, duration: 3.326s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.341 [-1.835, 0.778], loss: 23.680470, mean_absolute_error: 54.345001, mean_q: 108.743233\n",
      " 32501/50000: episode: 191, duration: 3.335s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.364 [-2.034, 0.676], loss: 17.858379, mean_absolute_error: 54.143723, mean_q: 108.732834\n",
      " 32701/50000: episode: 192, duration: 3.330s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.385 [-2.049, 0.570], loss: 25.364389, mean_absolute_error: 54.354427, mean_q: 108.837440\n",
      " 32901/50000: episode: 193, duration: 3.329s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.376 [-2.202, 0.528], loss: 28.179619, mean_absolute_error: 54.356857, mean_q: 108.762245\n",
      " 33101/50000: episode: 194, duration: 3.327s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.367 [-2.026, 0.910], loss: 16.646395, mean_absolute_error: 54.141357, mean_q: 108.654587\n",
      " 33301/50000: episode: 195, duration: 3.338s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.437 [-2.398, 0.589], loss: 36.628273, mean_absolute_error: 54.259132, mean_q: 108.571854\n",
      " 33501/50000: episode: 196, duration: 3.330s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.385 [-2.092, 0.849], loss: 16.533144, mean_absolute_error: 54.487495, mean_q: 109.392967\n",
      " 33701/50000: episode: 197, duration: 3.334s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.363 [-2.079, 0.863], loss: 27.827673, mean_absolute_error: 54.445744, mean_q: 109.019821\n",
      " 33901/50000: episode: 198, duration: 3.323s, episode steps: 200, steps per second: 60, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.395 [-2.265, 0.729], loss: 16.112968, mean_absolute_error: 54.310978, mean_q: 109.247482\n",
      " 34096/50000: episode: 199, duration: 3.248s, episode steps: 195, steps per second: 60, episode reward: 195.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: -0.436 [-2.400, 0.595], loss: 26.454275, mean_absolute_error: 54.343510, mean_q: 108.926643\n",
      " 34281/50000: episode: 200, duration: 3.093s, episode steps: 185, steps per second: 60, episode reward: 185.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: -0.455 [-2.407, 0.563], loss: 18.690617, mean_absolute_error: 54.630005, mean_q: 109.835709\n",
      " 34462/50000: episode: 201, duration: 3.014s, episode steps: 181, steps per second: 60, episode reward: 181.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: -0.436 [-2.408, 0.614], loss: 30.165344, mean_absolute_error: 54.706860, mean_q: 109.662361\n",
      " 34660/50000: episode: 202, duration: 3.304s, episode steps: 198, steps per second: 60, episode reward: 198.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.429 [-2.415, 0.811], loss: 17.239576, mean_absolute_error: 54.790882, mean_q: 110.101517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 34839/50000: episode: 203, duration: 2.993s, episode steps: 179, steps per second: 60, episode reward: 179.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.438 [-2.407, 0.806], loss: 19.130663, mean_absolute_error: 55.078648, mean_q: 110.784630\n",
      " 35020/50000: episode: 204, duration: 3.014s, episode steps: 181, steps per second: 60, episode reward: 181.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.429 [-2.402, 0.840], loss: 25.831694, mean_absolute_error: 55.429688, mean_q: 111.405655\n",
      " 35203/50000: episode: 205, duration: 3.043s, episode steps: 183, steps per second: 60, episode reward: 183.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: -0.436 [-2.401, 0.738], loss: 27.713648, mean_absolute_error: 55.254654, mean_q: 110.883904\n",
      " 35388/50000: episode: 206, duration: 3.083s, episode steps: 185, steps per second: 60, episode reward: 185.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.433 [-2.406, 0.570], loss: 19.726450, mean_absolute_error: 55.394794, mean_q: 111.442596\n",
      " 35581/50000: episode: 207, duration: 3.217s, episode steps: 193, steps per second: 60, episode reward: 193.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: -0.425 [-2.407, 0.621], loss: 21.823055, mean_absolute_error: 55.408840, mean_q: 111.333229\n",
      " 35764/50000: episode: 208, duration: 3.042s, episode steps: 183, steps per second: 60, episode reward: 183.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: -0.418 [-2.412, 0.749], loss: 24.631495, mean_absolute_error: 55.626194, mean_q: 111.806274\n",
      " 35956/50000: episode: 209, duration: 3.197s, episode steps: 192, steps per second: 60, episode reward: 192.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.418 [-2.407, 0.915], loss: 21.535759, mean_absolute_error: 55.778736, mean_q: 112.082451\n",
      " 36129/50000: episode: 210, duration: 2.889s, episode steps: 173, steps per second: 60, episode reward: 173.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.436 [-2.418, 0.611], loss: 15.468990, mean_absolute_error: 55.841854, mean_q: 112.497169\n",
      " 36306/50000: episode: 211, duration: 2.937s, episode steps: 177, steps per second: 60, episode reward: 177.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: -0.421 [-2.413, 0.715], loss: 16.253723, mean_absolute_error: 56.289581, mean_q: 113.306435\n",
      " 36481/50000: episode: 212, duration: 2.925s, episode steps: 175, steps per second: 60, episode reward: 175.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.425 [-2.405, 0.892], loss: 23.635168, mean_absolute_error: 56.095558, mean_q: 112.866829\n",
      " 36671/50000: episode: 213, duration: 3.155s, episode steps: 190, steps per second: 60, episode reward: 190.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.422 [-2.411, 0.599], loss: 19.341455, mean_absolute_error: 56.664383, mean_q: 113.986946\n",
      " 36857/50000: episode: 214, duration: 3.108s, episode steps: 186, steps per second: 60, episode reward: 186.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.435 [-2.403, 0.760], loss: 28.806353, mean_absolute_error: 56.484985, mean_q: 113.337440\n",
      " 37041/50000: episode: 215, duration: 3.065s, episode steps: 184, steps per second: 60, episode reward: 184.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.429 [-2.408, 0.598], loss: 19.370611, mean_absolute_error: 56.459599, mean_q: 113.546440\n",
      " 37227/50000: episode: 216, duration: 3.099s, episode steps: 186, steps per second: 60, episode reward: 186.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.425 [-2.403, 0.513], loss: 24.642904, mean_absolute_error: 56.424606, mean_q: 113.325745\n",
      " 37411/50000: episode: 217, duration: 3.062s, episode steps: 184, steps per second: 60, episode reward: 184.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.436 [-2.411, 0.553], loss: 22.677504, mean_absolute_error: 56.321808, mean_q: 113.110756\n",
      " 37585/50000: episode: 218, duration: 2.897s, episode steps: 174, steps per second: 60, episode reward: 174.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.428 [-2.404, 0.575], loss: 20.050720, mean_absolute_error: 56.422962, mean_q: 113.407959\n",
      " 37766/50000: episode: 219, duration: 3.013s, episode steps: 181, steps per second: 60, episode reward: 181.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.435 [-2.404, 0.801], loss: 26.772690, mean_absolute_error: 56.581364, mean_q: 113.620857\n",
      " 37948/50000: episode: 220, duration: 3.046s, episode steps: 182, steps per second: 60, episode reward: 182.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.438 [-2.415, 0.962], loss: 13.378404, mean_absolute_error: 56.540874, mean_q: 113.803871\n",
      " 38138/50000: episode: 221, duration: 3.158s, episode steps: 190, steps per second: 60, episode reward: 190.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.427 [-2.401, 0.629], loss: 22.409019, mean_absolute_error: 56.843014, mean_q: 114.150185\n",
      " 38329/50000: episode: 222, duration: 3.194s, episode steps: 191, steps per second: 60, episode reward: 191.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.413 [-2.410, 0.612], loss: 21.412067, mean_absolute_error: 56.721531, mean_q: 113.953705\n",
      " 38520/50000: episode: 223, duration: 3.174s, episode steps: 191, steps per second: 60, episode reward: 191.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: -0.426 [-2.407, 0.806], loss: 23.987810, mean_absolute_error: 56.842247, mean_q: 114.051895\n",
      " 38697/50000: episode: 224, duration: 2.954s, episode steps: 177, steps per second: 60, episode reward: 177.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.417 [-2.411, 0.823], loss: 22.407627, mean_absolute_error: 56.927910, mean_q: 114.498016\n",
      " 38870/50000: episode: 225, duration: 2.884s, episode steps: 173, steps per second: 60, episode reward: 173.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.437 [-2.403, 0.570], loss: 31.923529, mean_absolute_error: 56.878174, mean_q: 114.044632\n",
      " 39029/50000: episode: 226, duration: 2.646s, episode steps: 159, steps per second: 60, episode reward: 159.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.455 [-2.415, 0.794], loss: 32.796696, mean_absolute_error: 56.931229, mean_q: 114.161209\n",
      " 39209/50000: episode: 227, duration: 2.999s, episode steps: 180, steps per second: 60, episode reward: 180.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.431 [-2.414, 0.800], loss: 24.865114, mean_absolute_error: 56.806244, mean_q: 114.055870\n",
      " 39405/50000: episode: 228, duration: 3.262s, episode steps: 196, steps per second: 60, episode reward: 196.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.415 [-2.417, 0.798], loss: 25.720165, mean_absolute_error: 56.724865, mean_q: 113.780014\n",
      " 39584/50000: episode: 229, duration: 2.980s, episode steps: 179, steps per second: 60, episode reward: 179.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.426 [-2.408, 0.826], loss: 36.266003, mean_absolute_error: 56.571568, mean_q: 113.314964\n",
      " 39762/50000: episode: 230, duration: 2.969s, episode steps: 178, steps per second: 60, episode reward: 178.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.442 [-2.411, 0.572], loss: 21.469074, mean_absolute_error: 56.457333, mean_q: 113.479492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 39959/50000: episode: 231, duration: 3.278s, episode steps: 197, steps per second: 60, episode reward: 197.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.403 [-2.405, 0.800], loss: 20.253801, mean_absolute_error: 56.447453, mean_q: 113.411484\n",
      " 40131/50000: episode: 232, duration: 2.869s, episode steps: 172, steps per second: 60, episode reward: 172.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.434 [-2.417, 0.631], loss: 29.556538, mean_absolute_error: 56.340328, mean_q: 112.983192\n",
      " 40312/50000: episode: 233, duration: 3.010s, episode steps: 181, steps per second: 60, episode reward: 181.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: -0.441 [-2.405, 0.637], loss: 24.649559, mean_absolute_error: 56.263988, mean_q: 112.916267\n",
      " 40487/50000: episode: 234, duration: 2.916s, episode steps: 175, steps per second: 60, episode reward: 175.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.435 [-2.405, 0.639], loss: 18.658178, mean_absolute_error: 56.461929, mean_q: 113.560555\n",
      " 40649/50000: episode: 235, duration: 2.702s, episode steps: 162, steps per second: 60, episode reward: 162.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.462 [-2.414, 0.555], loss: 24.436554, mean_absolute_error: 56.424309, mean_q: 113.349800\n",
      " 40823/50000: episode: 236, duration: 2.887s, episode steps: 174, steps per second: 60, episode reward: 174.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.447 [-2.411, 0.533], loss: 15.840307, mean_absolute_error: 56.491116, mean_q: 113.569527\n",
      " 40994/50000: episode: 237, duration: 2.859s, episode steps: 171, steps per second: 60, episode reward: 171.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.444 [-2.410, 0.807], loss: 17.036512, mean_absolute_error: 56.343990, mean_q: 113.228889\n",
      " 41191/50000: episode: 238, duration: 3.281s, episode steps: 197, steps per second: 60, episode reward: 197.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.418 [-2.403, 0.783], loss: 24.671091, mean_absolute_error: 55.994251, mean_q: 112.391144\n",
      " 41387/50000: episode: 239, duration: 3.263s, episode steps: 196, steps per second: 60, episode reward: 196.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.402 [-2.408, 0.795], loss: 19.664764, mean_absolute_error: 56.244431, mean_q: 112.959320\n",
      " 41570/50000: episode: 240, duration: 3.042s, episode steps: 183, steps per second: 60, episode reward: 183.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.432 [-2.405, 0.783], loss: 17.715010, mean_absolute_error: 55.853195, mean_q: 112.192429\n",
      " 41752/50000: episode: 241, duration: 3.038s, episode steps: 182, steps per second: 60, episode reward: 182.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.428 [-2.406, 0.840], loss: 15.031855, mean_absolute_error: 55.845020, mean_q: 112.262871\n",
      " 41938/50000: episode: 242, duration: 3.097s, episode steps: 186, steps per second: 60, episode reward: 186.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.433 [-2.413, 1.016], loss: 14.654816, mean_absolute_error: 55.593319, mean_q: 111.876030\n",
      " 42104/50000: episode: 243, duration: 2.754s, episode steps: 166, steps per second: 60, episode reward: 166.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.450 [-2.415, 0.799], loss: 13.831152, mean_absolute_error: 55.849297, mean_q: 112.460281\n",
      " 42263/50000: episode: 244, duration: 2.659s, episode steps: 159, steps per second: 60, episode reward: 159.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.450 [-2.403, 0.472], loss: 19.486015, mean_absolute_error: 55.838413, mean_q: 112.229042\n",
      " 42436/50000: episode: 245, duration: 2.881s, episode steps: 173, steps per second: 60, episode reward: 173.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.443 [-2.404, 0.624], loss: 16.550653, mean_absolute_error: 55.744328, mean_q: 112.209496\n",
      " 42611/50000: episode: 246, duration: 2.904s, episode steps: 175, steps per second: 60, episode reward: 175.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.456 [-2.410, 0.873], loss: 23.205122, mean_absolute_error: 55.631561, mean_q: 111.794510\n",
      " 42784/50000: episode: 247, duration: 2.891s, episode steps: 173, steps per second: 60, episode reward: 173.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.454 [-2.409, 0.566], loss: 20.435799, mean_absolute_error: 55.736645, mean_q: 111.963531\n",
      " 42953/50000: episode: 248, duration: 2.808s, episode steps: 169, steps per second: 60, episode reward: 169.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.464 [-2.408, 0.826], loss: 20.405897, mean_absolute_error: 55.432716, mean_q: 111.496498\n",
      " 43120/50000: episode: 249, duration: 2.779s, episode steps: 167, steps per second: 60, episode reward: 167.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.464 [-2.409, 0.872], loss: 27.159981, mean_absolute_error: 55.525192, mean_q: 111.513847\n",
      " 43305/50000: episode: 250, duration: 3.087s, episode steps: 185, steps per second: 60, episode reward: 185.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.441 [-2.407, 0.743], loss: 21.513939, mean_absolute_error: 55.252460, mean_q: 110.970314\n",
      " 43479/50000: episode: 251, duration: 2.901s, episode steps: 174, steps per second: 60, episode reward: 174.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.454 [-2.410, 0.623], loss: 12.429867, mean_absolute_error: 55.059155, mean_q: 110.949959\n",
      " 43660/50000: episode: 252, duration: 3.010s, episode steps: 181, steps per second: 60, episode reward: 181.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.440 [-2.405, 0.549], loss: 17.272030, mean_absolute_error: 54.838818, mean_q: 110.298386\n",
      " 43830/50000: episode: 253, duration: 2.842s, episode steps: 170, steps per second: 60, episode reward: 170.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.450 [-2.407, 0.887], loss: 16.748898, mean_absolute_error: 54.791832, mean_q: 110.377472\n",
      " 44004/50000: episode: 254, duration: 2.896s, episode steps: 174, steps per second: 60, episode reward: 174.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.447 [-2.406, 0.731], loss: 17.608702, mean_absolute_error: 55.008514, mean_q: 110.699150\n",
      " 44192/50000: episode: 255, duration: 3.132s, episode steps: 188, steps per second: 60, episode reward: 188.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.412 [-2.406, 0.586], loss: 17.100037, mean_absolute_error: 54.981838, mean_q: 110.601181\n",
      " 44351/50000: episode: 256, duration: 2.647s, episode steps: 159, steps per second: 60, episode reward: 159.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.452 [-2.409, 0.827], loss: 21.763205, mean_absolute_error: 54.731377, mean_q: 110.109772\n",
      " 44522/50000: episode: 257, duration: 2.837s, episode steps: 171, steps per second: 60, episode reward: 171.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.457 [-2.413, 0.583], loss: 19.714741, mean_absolute_error: 54.875538, mean_q: 110.389549\n",
      " 44702/50000: episode: 258, duration: 2.999s, episode steps: 180, steps per second: 60, episode reward: 180.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.420 [-2.406, 0.738], loss: 24.756693, mean_absolute_error: 54.575428, mean_q: 109.606186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 44877/50000: episode: 259, duration: 2.916s, episode steps: 175, steps per second: 60, episode reward: 175.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.430 [-2.418, 0.631], loss: 21.155750, mean_absolute_error: 54.315792, mean_q: 109.268463\n",
      " 45045/50000: episode: 260, duration: 2.800s, episode steps: 168, steps per second: 60, episode reward: 168.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.457 [-2.414, 0.658], loss: 21.106016, mean_absolute_error: 54.115910, mean_q: 108.695030\n",
      " 45215/50000: episode: 261, duration: 2.840s, episode steps: 170, steps per second: 60, episode reward: 170.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.451 [-2.405, 0.747], loss: 14.532324, mean_absolute_error: 53.951481, mean_q: 108.687294\n",
      " 45387/50000: episode: 262, duration: 2.866s, episode steps: 172, steps per second: 60, episode reward: 172.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.436 [-2.401, 0.510], loss: 13.931101, mean_absolute_error: 53.869846, mean_q: 108.514923\n",
      " 45566/50000: episode: 263, duration: 2.984s, episode steps: 179, steps per second: 60, episode reward: 179.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.429 [-2.402, 0.658], loss: 22.072887, mean_absolute_error: 53.929493, mean_q: 108.449226\n",
      " 45750/50000: episode: 264, duration: 3.061s, episode steps: 184, steps per second: 60, episode reward: 184.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.428 [-2.404, 0.662], loss: 16.067493, mean_absolute_error: 53.309147, mean_q: 107.209129\n",
      " 45938/50000: episode: 265, duration: 3.132s, episode steps: 188, steps per second: 60, episode reward: 188.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.413 [-2.412, 0.584], loss: 19.956104, mean_absolute_error: 53.341835, mean_q: 107.253639\n",
      " 46105/50000: episode: 266, duration: 2.780s, episode steps: 167, steps per second: 60, episode reward: 167.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.440 [-2.402, 1.007], loss: 20.331295, mean_absolute_error: 53.405876, mean_q: 107.428856\n",
      " 46273/50000: episode: 267, duration: 2.799s, episode steps: 168, steps per second: 60, episode reward: 168.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.448 [-2.403, 0.746], loss: 21.249081, mean_absolute_error: 53.323601, mean_q: 107.170563\n",
      " 46442/50000: episode: 268, duration: 2.817s, episode steps: 169, steps per second: 60, episode reward: 169.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.448 [-2.411, 0.551], loss: 20.177082, mean_absolute_error: 53.046082, mean_q: 106.700203\n",
      " 46606/50000: episode: 269, duration: 2.730s, episode steps: 164, steps per second: 60, episode reward: 164.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.475 [-2.405, 0.537], loss: 16.116949, mean_absolute_error: 52.632282, mean_q: 105.921539\n",
      " 46786/50000: episode: 270, duration: 3.000s, episode steps: 180, steps per second: 60, episode reward: 180.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.458 [-2.404, 0.586], loss: 17.765125, mean_absolute_error: 52.699078, mean_q: 106.062630\n",
      " 46946/50000: episode: 271, duration: 2.666s, episode steps: 160, steps per second: 60, episode reward: 160.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.471 [-2.416, 0.709], loss: 19.162535, mean_absolute_error: 52.554699, mean_q: 105.626877\n",
      " 47128/50000: episode: 272, duration: 3.032s, episode steps: 182, steps per second: 60, episode reward: 182.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.422 [-2.417, 0.656], loss: 14.673418, mean_absolute_error: 52.159447, mean_q: 104.983498\n",
      " 47281/50000: episode: 273, duration: 2.551s, episode steps: 153, steps per second: 60, episode reward: 153.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.458 [-2.412, 0.535], loss: 20.877222, mean_absolute_error: 52.275387, mean_q: 105.183311\n",
      " 47445/50000: episode: 274, duration: 2.731s, episode steps: 164, steps per second: 60, episode reward: 164.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.440 [-2.413, 0.634], loss: 14.600113, mean_absolute_error: 52.310982, mean_q: 105.263550\n",
      " 47612/50000: episode: 275, duration: 2.783s, episode steps: 167, steps per second: 60, episode reward: 167.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.442 [-2.407, 0.616], loss: 18.957275, mean_absolute_error: 51.928833, mean_q: 104.440575\n",
      " 47786/50000: episode: 276, duration: 2.899s, episode steps: 174, steps per second: 60, episode reward: 174.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.447 [-2.405, 0.612], loss: 15.833053, mean_absolute_error: 51.880451, mean_q: 104.271217\n",
      " 47956/50000: episode: 277, duration: 2.826s, episode steps: 170, steps per second: 60, episode reward: 170.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.437 [-2.411, 0.618], loss: 9.378018, mean_absolute_error: 52.091038, mean_q: 104.956879\n",
      " 48126/50000: episode: 278, duration: 2.838s, episode steps: 170, steps per second: 60, episode reward: 170.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.455 [-2.405, 0.797], loss: 19.620039, mean_absolute_error: 51.797276, mean_q: 104.123657\n",
      " 48306/50000: episode: 279, duration: 2.998s, episode steps: 180, steps per second: 60, episode reward: 180.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.426 [-2.409, 0.708], loss: 16.215429, mean_absolute_error: 51.648556, mean_q: 103.831024\n",
      " 48465/50000: episode: 280, duration: 2.648s, episode steps: 159, steps per second: 60, episode reward: 159.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.457 [-2.401, 0.552], loss: 11.803797, mean_absolute_error: 51.297497, mean_q: 103.331100\n",
      " 48632/50000: episode: 281, duration: 2.778s, episode steps: 167, steps per second: 60, episode reward: 167.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.436 [-2.401, 0.712], loss: 14.092346, mean_absolute_error: 51.528015, mean_q: 103.751190\n",
      " 48805/50000: episode: 282, duration: 2.886s, episode steps: 173, steps per second: 60, episode reward: 173.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.447 [-2.412, 0.712], loss: 18.655725, mean_absolute_error: 50.535995, mean_q: 101.589439\n",
      " 48972/50000: episode: 283, duration: 2.783s, episode steps: 167, steps per second: 60, episode reward: 167.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.439 [-2.415, 0.583], loss: 12.949043, mean_absolute_error: 50.892620, mean_q: 102.475731\n",
      " 49151/50000: episode: 284, duration: 2.982s, episode steps: 179, steps per second: 60, episode reward: 179.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.430 [-2.410, 0.601], loss: 13.697396, mean_absolute_error: 50.823330, mean_q: 102.206009\n",
      " 49320/50000: episode: 285, duration: 2.817s, episode steps: 169, steps per second: 60, episode reward: 169.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.453 [-2.408, 0.689], loss: 16.927856, mean_absolute_error: 50.837429, mean_q: 102.199310\n",
      " 49488/50000: episode: 286, duration: 2.798s, episode steps: 168, steps per second: 60, episode reward: 168.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.433 [-2.417, 0.636], loss: 14.921361, mean_absolute_error: 50.716644, mean_q: 101.996452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 49668/50000: episode: 287, duration: 2.997s, episode steps: 180, steps per second: 60, episode reward: 180.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.424 [-2.408, 0.809], loss: 18.064224, mean_absolute_error: 50.541508, mean_q: 101.627213\n",
      " 49830/50000: episode: 288, duration: 2.692s, episode steps: 162, steps per second: 60, episode reward: 162.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.458 [-2.410, 0.583], loss: 9.740383, mean_absolute_error: 49.986721, mean_q: 100.574921\n",
      "done, took 833.853 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15f2a0ebef0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy = EpsGreedyQPolicy()\n",
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10,\n",
    "target_model_update=1e-2, policy=policy)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "\n",
    "# Okay, now it's time to learn something! We visualize the training here for show, but this slows down training quite a lot. \n",
    "dqn.fit(env, nb_steps=50000, visualize=True, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: 189.000, steps: 189\n",
      "Episode 2: reward: 181.000, steps: 181\n",
      "Episode 3: reward: 173.000, steps: 173\n",
      "Episode 4: reward: 161.000, steps: 161\n",
      "Episode 5: reward: 181.000, steps: 181\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15f2fb590f0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.test(env, nb_episodes=5, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
